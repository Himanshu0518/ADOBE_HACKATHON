{
  "title": "Attention Is All You Need",
  "outline": [
    {
      "level": "H2",
      "text": "Abstract",
      "page": 1
    },
    {
      "level": "H2",
      "text": "Introduction",
      "page": 2
    },
    {
      "level": "H2",
      "text": "Background",
      "page": 2
    },
    {
      "level": "H2",
      "text": "Model Architecture",
      "page": 2
    },
    {
      "level": "H2",
      "text": "Encoder and Decoder Stacks",
      "page": 3
    },
    {
      "level": "H3",
      "text": "Encoder:",
      "page": 3
    },
    {
      "level": "H1",
      "text": "Decoder:",
      "page": 3
    },
    {
      "level": "H2",
      "text": "Attention",
      "page": 3
    },
    {
      "level": "H2",
      "text": "Position-wise Feed-Forward Networks",
      "page": 5
    },
    {
      "level": "H2",
      "text": "Embeddings and Softmax",
      "page": 5
    },
    {
      "level": "H2",
      "text": "Positional Encoding",
      "page": 6
    },
    {
      "level": "H2",
      "text": "Training",
      "page": 7
    },
    {
      "level": "H2",
      "text": "Training Data and Batching",
      "page": 7
    },
    {
      "level": "H2",
      "text": "Hardware and Schedule",
      "page": 7
    },
    {
      "level": "H2",
      "text": "Optimizer",
      "page": 7
    },
    {
      "level": "H2",
      "text": "Regularization",
      "page": 7
    },
    {
      "level": "H2",
      "text": "Label Smoothing",
      "page": 8
    },
    {
      "level": "H2",
      "text": "Results",
      "page": 8
    },
    {
      "level": "H2",
      "text": "Machine Translation",
      "page": 8
    },
    {
      "level": "H2",
      "text": "Model Variations",
      "page": 8
    },
    {
      "level": "H2",
      "text": "English Constituency Parsing",
      "page": 9
    },
    {
      "level": "H2",
      "text": "Conclusion",
      "page": 10
    },
    {
      "level": "H3",
      "text": "Acknowledgements",
      "page": 10
    },
    {
      "level": "H2",
      "text": "References",
      "page": 10
    },
    {
      "level": "H2",
      "text": "[10]Alex Graves.",
      "page": 11
    },
    {
      "level": "H2",
      "text": "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts",
      "page": 12
    },
    {
      "level": "H2",
      "text": "Attention Visualizations",
      "page": 13
    },
    {
      "level": "H1",
      "text": "<EOS>",
      "page": 14
    },
    {
      "level": "H1",
      "text": "<EOS>",
      "page": 15
    }
  ]
}